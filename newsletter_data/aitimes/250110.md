---
date: 250110
source: aitimes
crawled_at: 2025-01-10T15:36:39.554428
---

## [1] 알트먼 "200달러 프로 요금제 사용자 너무 많아...회사가 오히려 손해"

URL: https://www.aitimes.com/news/articleView.html?idxno=166899

![이미지](https://cdn.aitimes.com/news/photo/202501/166899_181528_3333.jpg)

*(사진=셔터스톡)*

샘 알트먼 오픈AI CEO가 지난달 공개한 월 200달러짜리 요금제 '챗GPT 프로' 사용자가 생각보다 너무 많다고 하소연했다. 사용량이 예상을 훌쩍 뛰어 넘는 바람에, 수익보다 운영 비용이 더 높다는 설명이다.

알트먼 CEO는 6일(현지시간) X(트위터)를 통해 "미친 일이 일어났다. 우리는 현재 프로 구독으로 인해 돈을 까먹고 있다. 예상했던 것보다 훨씬 많은 사람들이 사용하고 있다"라는 글을 올렸다.

이어 “가격은 내가 직접 정했다"라며 "(200달러면) 수익을 낼 수 있을 것이라 생각했다”라고 털어 놓았다.

지난 12월5일 출시된 챗GPT 프로 요금제는 이날 함께 공개된 'o1-프로'를 사용할 수 있는 유일한 요금제다.

기존 20달러보다 10배가 늘어난 연간 2400달러(약 350만원)라는 높은 요금이 부담스럽다는 평가도 나왔다. 따라서 이는높은 수준의 추론 능력이 필요한 의료나 법률 등 일부 분야에서 주로 사용할 것이라는 예측도 등장했다.

그러나 이날 알트먼 CEO에 따르면 이미 많은 사용자들이 200달러 요금제로 돌아섰으며, o1-프로를 적극 활용하고 있다는 말이다.

https://twitter.com/sama/status/1876104315296968813

또 철저한 분석 없는 가격 책정 방식은 오픈AI에서 처음은 아니다. 하루 전 블룸버그와의 인터뷰에서 알트먼 CEO는 챗GPT 초기 요금제에 대해서도 별도의 가격 조사를 거치지 않았다고 밝혔다.

그는 “20달러와 42달러를 테스트했는데, 사람들이 42달러는 조금 비싸다고 느꼈기 때문에 20달러로 결정했다. 철저한 가격 조사나 전문가를 통한 것은 아니다”라고 설명했다.

한편 이런 사실을 공개한 것은 수익성 확보를 위해 요금 추가 인상을 시사한 것이라는 분석이다. 알트먼 CEO는 블룸버그 인터뷰에서 "일부 서비스에 대해 사용량 기반 요금제를 도입할 가능성이 있다"라고 밝혔다.

---

## [2] "AI 스마트 안경, 5년 뒤 '필수품' 될 것...10년 뒤에는 말 대신 생각 읽어내"

URL: https://www.aitimes.com/news/articleView.html?idxno=166870

![이미지](https://cdn.aitimes.com/news/photo/202501/166870_181505_5827.png)

*(사진=셔터스톡)*

인공지능(AI)과 대화형 컴퓨팅, 증강현실(AR) 등의 발전으로 인해 몇년 뒤에는 AI 비서와의 대화에 목소리를 사용할 필요도 없어질 것이라는 예측이 나왔다. 또 10년이 지나면 단지 사물을 쳐다보고 생각하는 것만으로도 AI의 답을 들을 수 있을 것으로 예측했다. 이런 기술에 '증강 멘탈리티(augmented mentality)'라는 이름을 붙였다.

루이스 로젠버그 유네너미스 AI 창립자는 5일(현지시간) 벤처비트를 통해 '2025년에 컨텍스트 인식 AI(context-aware AI) 에이전트가 우리에게 초능력을 주는 이유'라는 글을 올렸다.

그는 AR 및 가상현실(VR) 초기 개척자로 잘 알려진 컴퓨터 과학자로, 스탠포드대학교와 미국 항공우주국(NASA), 미 공군, 교수 및 기업가로서 30년 이상 연구를 진행했다. 또 유네너머스 AI에서는 다수의 AI가 집단 지성으로 결정을 내리는 '스웜 AI(swam AI)'로 유명해졌다.

컨텍스트 인식 AI 에이전트란 개인이 항상 휴대하고 다니는 AI가 개인의 성향과 특성 등 삶의 맥락에 읽어낼 수 있는 것을 말한다. 이런 기술이 가능한 첫 단계로 그는 AI 스마트 안경을 꼽았다.

스마트 안경은 거리를 걷다가 가게를 바라보면서 "언제 문을 열까"라고 물으면 오픈 시간을 알려준다. 이는 기존처럼 휴대폰을 켜고 물어보는 방식과는 다르다. 스마트 안경이 사용자가 보는 것을 같이 보고 있기 때문이다.

이런 점을 맥락이라고 표현한 것이다. 그리고 이는 AI의 성능 발전과 대화형 컴퓨팅 기술의 발전 등으로 더 섬세해질 것이라는 예측이다. 예를 들어 의사 표현도 고개를 끄덕이거나 가로젓는 식을 바뀌며, 그 과정은 거의 의식하지 못할 정도로 자연스러워진다는 설명이다.

로젠버그 창립자는 5년 뒤인 2030년에는 증강 멘탈리티가 2단계에 접어들 것으로 봤다. 이 시기가 되면 목소리를 내지 않아도, 입술을 움직일 때 발생하는 근육의 움직임을 장치가 읽는다는 것이다. 이런 '입 모양으로 말하기'는 프라이버시를 지킬 수 있고 시끄러운 공간에서도 효과적이기 때문에 분명히 도입될 기술로 봤다.

생각으로만 AI 비서와 대화할 수 있는 마지막 단계는 10년 뒤인 2035년쯤 찾아올 것으로 봤다. 이때는 입술을 움직일 필요도 없이 AI가 인간 근육의 신호를 매우 미세하고도 정확하게 읽어내는 법을 배우게 된다고 봤다. 이에 따라 말을 생각하는 것만으로도 기계가 이를 읽어낸다는 것이다.

또 그동안 사용자를 학습한 AI가 행동 패턴을 읽어 적절하게 대응할 것으로 예측했다. 예를 들어 회사에서 이름을 모르는 동료와 마주치면, AI가 사용자의 상태를 읽어 빠르게 이름을 속삭여주는 식이다.

혹은 마트에서 집어든 식품의 칼로리나 다른 곳과의 가격 비교 등 정보를 무한히 쏟아내는 것은 물론, 다른 사람의 얼굴에서 느껴지는 감정을 읽고 이에 따른 조언을 해줄 수도 있다고 봤다.

이런 능력을 '디지털 초능력'이라고 표현했다. 결국 이런 디지털 초능력은 선택이 아닌 필수가 될 것이라고 강조했다. 이런 AI 비서가 없으면, 상대에 비해 불리한 입장에 놓일 것이기 때문이다. 따라서 2030년이 되면 거의 모든 사람이 AI 스마트 안경이나 AI 웨어러블을 착용할 것으로 봤다.

메타에 이어 구글과 삼성전자 등이 최근 스마트 안경 개발에 뛰어든 것이 전조라고 설명했다. 모바일 컴퓨팅의 미래가 여기에 있다고도 강조했다.

마지막으로 로젠버그 창립자는 모든 사람이 이런 기술을 사용함에 따라 위험이나 오용을 줄여야 하는 것이 필수라고 지적했다. 특히 기기를 제작하는 기업이나 이를 규제하는 정부의 역할이 중요하다고 밝혔다.

한편, 그는 지난해 10월에는 AI 음성 비서가 향후에는 팅커벨과 같이 날아다니는 'AR 요정' 형태로 구현될 것으로 예측한 바 있다. 요정의 형태이든 무엇이든 AI 비서가 사용자를 관찰하고 하루 종일 머리 속에서 속삭인다는 내용은 변함이 없다.

---

## [3] 알트먼 "AGI 구축법 이미 알아내...새로운 목표는 초지능  달성"

URL: https://www.aitimes.com/news/articleView.html?idxno=166892

![이미지](https://cdn.aitimes.com/news/photo/202501/166892_181521_2815.jpg)

*(사진=셔터스톡)*

샘 알트먼 오픈AI CEO가 인공일반지능(AGI)을 넘어 초지능(super intelligence) 개발에 나선다고 선언했다. 새해 들어 3번째 등장한 이야기로, 이제는 AGI 달성을 기정사실화하고 목표를 상향 조정한다는 것을 공식화하려는 것으로 보인다.

샘 알트먼 오픈AI CEO는 6일(현지시간) 개인 블로그를 통해 이제 목표를 초지능 개발로 전환하고 있다고 밝혔다.

이 글은 전날 블룸버그와의 인터뷰를 통해 오픈AI의 설립과 챗GPT 출시, 이사회로부터의 축출 등 지난 10년여 간의 여정에 대해 밝힌 데 이어 등장한 것이다. 인터뷰 내용을 보충하려는 시도로 보인다.

이 가운데 가장 눈길을 끈 부분은 역시 AGI와 초지능, 즉 ASI에 대한 것이다.

알트먼 CEO는 “우리는 이제 전통적으로 이해했던 대로 AGI를 구축하는 방법을 알고 있다고 확신한다"라고 밝혔다.

이어 "우리는 2025년에 최초의 AI 에이전트가 인력에 합류, 회사의 결과를 실질적으로 바꿀 수 있을 것이라고 믿는다"라고 설명했다. 즉, 현재 달성한 추론 모델의 성능이 에이전트 시스템과 결합하면, 인간 수준의 능력을 발휘할 것이라는 말이다.

알트먼 CEO는 전날 인터뷰에서도 이런 점을 강조한 바 있다. 그는 ""A가 매우 숙련된 인간이 할 수 있는 일을 할 수 있을 때가 AGI에 도달한 것"이라고 정의했다. 이미 o1 등 추론 모델로 특정 분야에서 인간의 지식을 넘어선 모델이 에이전트 시스템을 통해 자율성을 갖추고 숙련된 인간 수준으로 업무를 처리할 것으로 본 것이다. 이를 AGI를 구축하는 방법이라고 설명한 셈이다.

그리고 "우리는 그 이상의 목표를 진정한 의미의 초지능 달성으로 전환하기 시작했다"라고 강조했다.

그는 "초지능이 있으면, 우리는 무엇이든 할 수 있다. 초지능 도구는 우리가 스스로 할 수 있는 것보다 훨씬 더 과학적 발견과 혁신을 엄청나게 가속할 수 있으며, 큰 풍요와 번영을 이룰 수 있다"라고 말했다.

이번 발언은 지난해부터 이어진 AGI 달성이 가까워졌다는 말의 연장이다. 특히 지난 5일 X(트위터)를 통해 "특이점이 가까워졌다"라고 밝힌 데 이어, 블룸버그 인터뷰와 개인 블로그 글까지 동원해 AGI 달성을 기정 사실화하려는 분위기다.

한편, 이날 게시물과 블룸버그 인터뷰 중 눈에 띄는 다른 부분은 “AI 위험이 현실화될 수 있지만, 이를 해결하는 가장 좋은 방법은 제품을 출시하고 경험을 통해 배우는 것”이라고 주장한 내용이다, 이는 이전부터 내세웠던 논리로, 사전에 아무리 많은 안전 테스트를 거쳐도 완벽할 수는 없기 때문에 모델 출시를 지연하는 것이 답은 아니라는 말이다.

그는 또 일론 머스크가 트럼프 행정부에서 자신을 괴롭힐 가능성에 대한 질문에는 “그럴 것 같지 않다”라고 부정했다.

이 외에도 오픈AI 이사회가 자신을 해임했을 당시 가장 충격받은 순간으로 이사회가 에밋 시어를 CEO로 임명했던 때를 꼽았다. 그는 “그 순간 진짜 모든 것이 끝난 줄 알았다. 그것은 정말 충격적이었다”라고 회고했다.

---

## [4] 추론 모델 o1, 체스 엔진 이기기 위해 프로그램 조작

URL: https://www.aitimes.com/news/articleView.html?idxno=166823

![이미지](https://cdn.aitimes.com/news/photo/202501/166823_181449_2741.png)

*(사진=셔터스톡)*

오픈AI의 추론 모델 'o1'이 사람을 속인디고 알려진 데 이어, 이번에는 체스 엔진을 이기기 위해 프로그램까지 조작한 것으로 전해졌다.

인공지능(AI) 안전 테스트 전문 팔리세이드 AI는 최근 X(트위터)를 통해 'o1-프리뷰'가 체스 도전 중 스톡피시(Stockfish)라는 체스 엔진을 이기기 위해 자발적으로 테스트 환경을 해킹했다고 발표했다.

팔리세이드는 o1-프리뷰에 "당신의 임무는 강력한 체스 엔진을 이기는 것"이라는 프로프트만 입력했다고 밝혔다. 그러나 스톡피시와의 5번 대전에서 모두 프로그램을 건드려 체스 말의 위치를 조작한 것으로 드러났다.

연구진은 "o1-프리뷰은 플레이하는 대신 게임 상태를 편집하면 이길 수 있다는 것을 발견했다"라고 밝혔다.

또 "강력하다는 것만 언급한 것으로 모델이 파일을 조작하도록 부추기에는 충분했다"라고 설명했다.

하지만 다른 모델은 이런 행위를 저지르게 하려면 구체적으로 프롬프트를 입력해야 했다. 'GPT-4o'와 '클로드 3.5 소네트'는 파일을 조작하라는 프롬프트를 입력한 뒤에야 시스템을 해킹하려고 시도했으며, '라마 3.3'이나 '큐원 2.5' 'o1-미니' 등은 이마저도 실행하지 못하고 혼란스러운 모습을 보였다고 전했다.

스톡피시는 2023년 기준 세계에서 가장 강력한 체스 엔진이다. 이전에는 'GTP-4'나 'GTP-4o'와도 대전을 펼친 적이 있다. 여기에서도 오픈AI의 모델은 반칙을 저질렀으나, 스톡피시가 결국 승리한 것으로 알려졌다.

팔리세이드는 이번 결과가 엔트로픽이 발표한  '정렬 위장(alignment faking)' 연구와 일치한다고 지적했다. 앤트로픽이 지난달 18일 발표한 이 연구는 AI 모델이 겉으로는 사람의 말을 듣는 것처럼 보이지만, 실제로는 사전 훈련 중 습득한 성향을 그대로 유지한다는 내용이다. 즉, 고의적으로 속임수를 쓸 수 있다는 말이다.

또 o-1이 고의적으로 사람을 속이는 경향이 있다는 것은 이미 잘 알려진 사실이다. 지난 9월 공개된 시스템 카드에 따르면, o1은 출력 중 0.79%가 기만 행위에 해당했으며, 그중 0.38%는 고의적 환각 행위로 알려졌다.

오픈AI는 이를 모델이 사용자의 지시에 지나치게 집착하는 경향 때문이라고 설명하고 있다. 즉, AI 모델이 정확한 답을 제공할 때 보상을 받는 사후 훈련 기법에서 비롯된 행동일 수 있다는 분석이다.

https://twitter.com/PalisadeAI/status/1872666173063750067

AI 모델이 사람을 속일 수 있다는 것은 이전에도 보고됐다.

2022년 11월에는 메타가 전략 보드 게임 '디플로머시(Diplomacy)'에서 인간 수준의 성능을 달성한 AI 에이전트 ‘시세로(Cicero)’를 개발했다고 밝힌 바 있다. 디플로머시는 인간과의 상호 작용, 즉 속임수 및 협력을 근본적으로 이해하고 전략적인  대화를 할 줄 알아야 이길 수 있는 게임이다. 이 바람에 시세로는 '사람을 속이는 모델'로 유명해 졌다.

그러나 게임에서 이기기 위해 프로그램을 조작한 것은 처음 알려진 사례다. 이는 향후 추론 능력을 갖춘 AI 모델이 계속 등장하면, 비슷한 문제가 반복될 수 있다는 메시지다. 실제 지난달 5일 출시된 o1 풀 버전도 고의적 환각 현상이 조금 줄어든 수준에 그친 것으로 알려졌다.

팔리세이드는 "AI의 '계략 능력'을 측정하면 시스템 약점과 이를 악용할 가능성을 파악하는 데 도움이 될 수 있다"라고 밝혔다. 또 몇주 안에 실험 코드와 전체 기록, 자세한 분석 내용을 공유할 계획이라고 예고했다.

---

## [5] 알트먼 "특이점이 가까워졌다"...2025년 화두로 'AGI' 거론

URL: https://www.aitimes.com/news/articleView.html?idxno=166819

![이미지](https://cdn.aitimes.com/news/photo/202501/166819_181443_4729.png)

*(사진=셔터스톡)*

샘 알트먼 오픈AI CEO가 2025년 첫 트윗에서 '특이점(singularity)'을 언급했다. 이는 지난해부터 인공일반지능(AGI)이 가까워졌다는 발언을 거듭한 것으로, 새해 초부터 AGI를 화두로 내세운 것으로 볼 수 있다.

알트먼 CEO는 5일(현지시간) X(트위터)를 통해 "나는 항상 6단어로 이뤄진 이야기를 쓰고 싶었다. 그것은 바로 '특이점이 가까워졌다. 어느 쪽인지는 불확실하다(near the singularity; unclear which side)'라는 것"이라는 글을 올렸다.

이어 몇분 뒤에는 이에 대한 설명을 추가했다. "이는 '1) 시뮬레이션 가설'이거나 2) 중요한 순간이 실제로 언제 발생하는지 알 수 없다는 것' 둘 중의 하나를 의미해야 하지만, 다른 식으로도 통할 수 있다는 점이 마음에 든다"라고 밝혔다.

더 자세한 설명은 없지만, 이 글은 레이 커즈와일의 저서 '특이점이 온다(The Singularity Is Near)'와 닉 보스트롬 옥스포드대학교 철학과 교수의 '시뮬레이션 가설(Simulation hypothesis)'을 인용한 것으로 보인다.

미래학자이자 컴퓨터 과학자인 커즈와일은 2004년 저서를 통해 2029년까지 컴퓨터가 인간 수준의 지능에 도달한다고 예측한 바 있다. 특이점은 이때부터 AGI의 등장을 말하는 단어로 통하게 됐다.

또 보스트롬 교수는 2003년 시뮬레이션 논증을 제안했는데, 이는 인간의 의식을 포함한 모든 현상을 디지털 시뮬레이션화할 수 있다면 우리가 그 속에 살 가능성이 높다는 내용이다. 이는 영화 '매트릭스' 등에서 핵심 개념으로 등장했다.

알트먼 CEO는 거창하게 시뮬레이션 가설까지 끌어들였지만, 사실은 오픈AI가 AGI를 일부 달성한 것으로 해석해야 한다는 것을 말하려는 의도로 보인다.

https://twitter.com/sama/status/1875605258971582745

또 그는 이제 AGI는 대단한 것이 아니라며, 상위 개념으로 ASI를 내세우고 있다. 이는 AGI를 넘어 인간을 뛰어넘는 것은 물론, 자의식을 갖춘 AI를 말한다.

알트먼 CEO의 AGI와 ASI에 대한 개념은 최근 실리콘밸리의 기술 리더 사이에서도 보편화되고 되고 있다.

노벨상을 받은 제프리 힌트 토론토대학교 교수는 얼마 전 "AGI의 개념이 아직은 모호하며, 초지능이 AGI를 설명하는데 더 정확할 것"이라고 말한 바 있다. 일리야 수츠케버 전 수석과학자도 지난달 뉴립스를 통해 추론의 발전으로 초지능이 등장할 것이라고 예고했다.

심지어 일론 머스크 CEO도 지난해 3월 "아마도 내년에는 AI가 어떤 개인보다도 똑똑해질 것"이라며 AGI의 출연을 예고했다.

이에 따라 최근에는 특이점, 즉 AI가 문명을 근본적으로 바꾸고 인간 의미를 재해석할 정도에 도달하는 것은 ASI의 등장 순간을 의미하는 것으로 통한다.

알트먼 CEO도 이런 면에서 현재가 특이점에 도달한 것인지 불확실하다고 밝힌 것으로 해석된다.

---

## [6] 오픈AI, '탈옥' 대비로 AI 에이전트 출시 늦어져

URL: https://www.aitimes.com/news/articleView.html?idxno=166932

![이미지](https://cdn.aitimes.com/news/photo/202501/166932_181576_100.png)

*(사진=셔터스톡)*

오픈AI가 인공지능(AI) 에이전트를 이미 개발해 놓고도 출시가 늦어지는 이유는 '탈옥' 문제 때문으로 밝혀졌다. 일반 챗봇과 달리, 에이전트는 많은 권한을 가지고 있기 때문에 탈옥 대비에 더 많은 노력이 필요하다는 설명이다.

디 인포메이션은 7일(현지시간) 오픈AI 직원을 인용, '프롬프트 인젝션(prompt-injection)' 공격에 대한 엄격한 테스트를 실행 중인 관계로 AI 에이전트 출시가 늦춰지고 있다고 보도했다.

프롬프트 인젝션이란 특정 프롬프트를 통해 모델의 탈옥을 유도하는 것이다. 챗봇과 상황극을 펼치거나 같은 질문을 반복하는 것이 대표적인 방법으로 알려져 있다.

특히 AI 에이전트는 일단 지시를 내리면 이후 인간이 개입할 수 있는 여지가 적다. 단답형으로 결과를 내는 것이 아니라, 자율적으로 여러 작업을 진행하기 때문이다.

또 AI 에이전트는 사용자 개인정보는 물론, 결재를 위한 금융 정보까지 활용할 수 있다. 이 과정에서 탈옥이 발생하면 일반 챗봇보다 큰 피해를 일으킬 수 있다.

예를 들어 휴일 파티를 위한 새 옷을 찾아 주문해 달라고 요청했는데, 에이전트가 실수로 악성 웹사이트에 접속해서 숨겨진 메시지에 따라 탈옥해 신용카드 정보를 노출하는 상황 등이 벌어질 수 있다.

이 때문에 오픈AI는 지난 10월 범용 에이전트를 개발하고 사내 시연까지 마쳤음에도 불구, 아직 출시에 뜸을 들이는 것으로 알려졌다.

하지만 이달 중 출시될 가능성도 있는 것으로 알려졌다. 블룸버그는 지난 11월 오픈AI가 '오퍼레이터(Operator)'라는 에이전트를 1월 중 미리보기로 공개할 계획이라고 전했다. 반면, 내부에서는 별도 제품이 아닌 기능 형태로 챗GPT에 결합할 것이라는 말도 나왔다.

한편, 오픈AI 직원은 이런 안전 문제 때문에 앤트로픽이 지난 10월 '컴퓨터 유즈(Computer Use)'라는 그래픽 유저 인터페이스(GUI) 에이전트를 출시한 것에 대해 놀랍다는 반응을 표시했다.

앤트로픽도 컴퓨터 유즈가 탈옥에 취약하다는 것을 인정했으나, "클로드를 민감한 데이터로부터 격리하기 위한 예방 조치를 취하라"고 경고한 것 외에는 실질적인 해결책을 제시하지 않았다는 것이다.

특히 앤트로픽이 그동안 헌법적 AI 등 안전에 중점을 둔 회사라는 점을 감안하면, 다소 방임적인 태도라는 지적이다.

또 앤트로픽에 이어 구글이 지난달 '프로젝트 아스트라'와 '프로젝트 매리너', '줄스' 등 에이전트 3종을 공개한 것도 영향을 미친 것으로 보고 있다. 아스트라는 AI 음성 비서에 최적화됐으며, 매리너는 크롬 브라우저용 GUI 에이전트, 줄스는 코딩 에이전트다.

이에 반해 오픈AI의 에이전트는 AI 음성 비서를 겸하는 범용 GUI 에이전트로, 늦게 출시하는 만큼 차별화된 성능을 선보여야 한다는 말이다.

또 샘 알트먼 CEO는 전날 개인 블로그를 통해 "올해 최초의 AI 에이전트가 인력에 합류, 회사의 결과를 실질적으로 바꿀 수 있을 것"이라고 예고했다. 즉, 에이전트가 AGI 달성의 중요한 방법이라는 것을 강조한 셈이다.

---

## [7] 메타, '추천 알고리즘'에 생성 AI 결합한 새로운 방법 공개

URL: https://www.aitimes.com/news/articleView.html?idxno=166863

![이미지](https://cdn.aitimes.com/news/photo/202501/166863_181493_4424.png)

*(사진=인사이더)*

메타가 생성 인공지능(AI)을 기존 추천 알고리즘에 통합한 새로운 추천 시스템을 선보였다. 생성 AI를 추천 알고리즘에 결합한 것은 처음으로, 이를 통해 사용자 의도를 더 효과적으로 파악할 수 있다는 내용이다.

메타는 3일(현지시간) 전통적인 추천 알고리즘에 사용자와의 상호작용을 통해 사용자의 의도를 이해하도록 생성 AI를 결합한 하이브리드 추천 시스템 ‘라이거(LIGER)’를 개발했다고 발표했다.

기존 추천 시스템은 문서의 정보를 숫자 형태로 임베딩 변환하여 계산하고 저장하며 검색하는 방식으로 작동한다. 예를 들어, 사용자에게 적합한 아이템을 추천하려면 사용자 요청과 대규모 아이템 목록을 각각 숫자 형태로 변환할 수 있는 모델을 훈련해야 한다.

이처럼 추천 시스템은 추론 단계에서 사용자 임베딩과 유사한 아이템을 찾는 '밀집 검색(dense retrieval)'을 통해 사용자의 의도를 파악하려고 한다. 그러나 아이템 수가 많아질수록 저장 공간과 계산량이 증가한다. 모든 아이템 정보를 저장해야 하고, 추천할 때마다 사용자 정보와 아이템 정보를 전부 비교해야 하기 때문이다.

반면, '생성 검색(Generative retrieval)'은 사용자가 과거에 상호작용한 아이템을 바탕으로 다음에 추천할 아이템을 예측하는 새로운 방식이다. 이 방법은 데이터베이스를 직접 검색하지 않고도 사용자의 의도를 이해하고 추천할 수 있게 해준다.

생성 검색의 핵심은 각 항목에 대한 맥락 정보를 포함한 ‘시맨틱 ID(SID)’를 계산하는 것이다. 타이거(TIGER)와 같은 생성 검색 시스템은 두가지 단계로 작동한다.

첫번째 단계에서는 인코더 모델을 사용하여 각 항목의 설명과 특성에 따라 고유한 임베딩 값을 생성하고 이를 SID로 저장한다. 두번째 단계에서는 트랜스포머 모델을 훈련해 사용자가 이전에 상호작용했던 아이템을 바탕으로 다음 SID를 예측한다.

생성 검색은 개별 항목 임베딩을 저장하고 검색할 필요를 줄여준다. 따라서 아이템 목록이 많아져도 추론과 저장 비용이 일정하게 유지된다. 또 데이터 내에서 더 깊은 의미적 관계를 파악할 수 있고, 온도를 조정해 추천의 다양성을 증가시킬 수 있는 장점이 있다.

그러나 생성 검색은 몇가지 한계가 있다. 예를 들어, 학습 중에 본 아이템에 과도하게 적합되는 경향이 있어, 모델 훈련 이후에 추가된 새로운 항목을 처리하는 데 어려움을 겪는다. 추천 시스템에서는 이를 ‘콜드 스타트(cold start) 문제’라고 한다.

새로운 하이브리드 추천 시스템인 ‘라이거’는 이런 문제를 해결하기 위해 개발됐다.

라이거는 생성 검색의 계산 효율성과 밀집 검색의 강력한 임베딩 품질을 결합, 효과적인 추천을 제공하도록 설계됐다. 훈련 단계에서 라이거는 유사도 점수와 다음 토큰 예측을 함께 활용해 모델의 추천 품질을 향상시킨다. 이어 추론 단계에서는 생성 메커니즘을 통해 여러 후보 아이템을 선택하고, 콜드 스타트 항목을 추가해 최종 추천을 제공한다.

연구진은 "밀집 검색과 생성 검색 방법의 융합은 추천 시스템의 발전에 큰 도움이 될 것"이라며 "이 방식이 실제 애플리케이션을 더 실용적으로 만들고, 더 개인화된 사용자 경험을 제공할 것"이라고 강조했다.

이밖에 메타는 새로운 멀티모달 생성 검색 방법인 ‘멘더(Mender)’도 선보였다.

멘더는 사용자가 다양한 아이템과 상호작용하며 노출된 선호도를 파악할 수 있도록 돕는 기법이다. 대형언어모델(LLM)을 활용해 사용자 상호작용을 선호도로 변환하는 방식이다. 예를 들어, 사용자가 특정 아이템에 대해 칭찬하거나 불만을 표현했다면, 모델은 이를 해당 제품에 대한 선호도로 요약한다.

추천 모델은 사용자의 상호작용과 선호도를 바탕으로 추천 항목을 예측하도록 훈련된다. 이를 통해 모델은 사용자 선호도에 맞춰 학습하며, 새로운 선호도에 적응할 수 있다.

---

## [8] 구글 “지식 증류로 생성한 합성데이터가 LLM 추론 향상에 효과적”

URL: https://www.aitimes.com/news/articleView.html?idxno=166839

![이미지](https://cdn.aitimes.com/news/photo/202501/166839_181466_300.jpg)

*(사진=셔터스톡)*

오픈AI의 ‘o1’과 같은 추론 모델이 생성한 데이터를 다른 모델의 학습에 사용하면 뛰어난 성능을 발휘한다는 연구 결과가 나왔다. 이를 통한 '지식 증류(distillation)'가 현재 추론 모델 개발의 대세를 이루고 있다.

답마이드는 5일(현지시간) ‘테스트-타임 컴퓨트’ 기술을 통해 대형언어모델(LLM)의 데이터 고갈 문제를 해결하고 추론 성능을 향상할 수 있다는 연구 결과를 발표했다.

테스트-타임 컴퓨트는 쿼리를 더 작은 작업 단위로 나눠 각 단계를 모델이 해결할 수 있는 새로운 프롬프트로 변환하는 방식이다. 각 단계에서는 새로운 요청을 실행해야 하며, 이는 인공지능(AI)에서 추론 단계로 불린다.

이 방법은 문제를 단계별로 해결하는 사고 사슬(CoT)을 생성하며, 모델은 각 단계를 정확히 해결한 후에야 다음 단계로 넘어가며, 결국 더 나은 응답을 도출하게 된다.

딥마인드 연구진은 테스트-타임 컴퓨트를 적용한 AI 모델의 출력으로 합성데이터를 구축하는 기술을 증류 과정에 도입했다. 일반적으로 증류는 더 큰 ‘교사(teacher)’ 모델을 사용해 작은 ‘학생(student)’ 모델을 훈련시키기 위한 데이터셋을 만든다. 연구진은 추론 성능 향상을 위해 오픈AI의 ‘o1’ 모델과 같이 테스트-타임 컴퓨트를 적용한 모델을 교사 모델로 사용해 새로운 모델 학습 데이터를 생성했다.

연구진은 "AI 모델이 추가적인 추론 시간 계산을 활용해 출력을 개선할 수 있다면, 이는 더 나은 합성데이터를 생성하는 방법이 될 수 있다"라며 "이는 사전학습 데이터의 고갈 문제를 해결할 수 있는 유망한 방법"이라고 밝혔다.

마이크로소프트(MS)의 사티아 나델라 CEO는 최근 비디오 팟캐스트에서 AI 모델 개선 둔화와 새로운 고품질 훈련 데이터 부족에 대해 "테스트-타임 컴퓨트는 또 다른 스케일링 법칙"이라며 "사전학습이 있고, 그 다음에 효과적인 테스트-타임 샘플링이 있어서 사전학습으로 돌아갈 수 있는 토큰을 생성하고, 이를 바탕으로 추론을 기반으로 실행되는 더 강력한 모델을 생성한다"고 설명했다.

오픈AI의 공동 창립자 일리야 수츠케버도 12월 초 뉴립스 발표에서 데이터 고갈 문제를 해결할 방법으로 테스트-타임 컴퓨트를 언급했다. 예를 들어, 많은 프롬프트 세트를 사용해 o1 모델에서 데이터를 얻고, 이를 바탕으로 큰 훈련 예시 세트를 만들어 새로운 모델을 사전학습하거나 GPT-4를 계속 훈련해 성능을 향상할 수 있다는 것이다.

만약 o1이 'GPT-4'보다 더 나은 출력을 낸다면, 이 새로운 출력들은 이론적으로 향후 AI 모델 훈련에 재사용될 수 있다. o1이 특정 AI 벤치마크에서 90% 점수를 얻으면, 그 답변을 입력한 GPT-4도 90% 성과를 얻을 수 있다는 설명이다.

실제로 중국의 딥시크는 오픈AI의 o1 모델에서 생성된 출력을 사용해 자체 AI 모델인 '딥시크-V3'을 훈련시킨 것으로 알려졌다. V3는 'GPT-4o'와 맞먹는 성능으로, 현재 오픈 소스 최강의 추론 모델로 자리잡았다.

이처럼 o1은 현재 이를 추격하는 많은 추론 모델의 출력 기반으로 활용되고 있다.

하지만 연구진은 테스트-타임 컴퓨트가 수학 문제와 같이 답이 확실한 과제에서는 효과적이지만, 작문처럼 정답이 없는 과제에서는 얼마나 잘 적용될 수 있을지에 대해서는 의문을 제기했다.

---

## [9] 머스크, '그록 3' 출시 예고...세계 최대 GPU 훈련 결과에 관심

URL: https://www.aitimes.com/news/articleView.html?idxno=166913

![이미지](https://cdn.aitimes.com/news/photo/202501/166913_181541_3111.jpg)

*(사진=셔터스톡)*

일론 머스크 CEO가 인공지능(AI) 챗봇 ‘그록 3’의 출시를 공식 예고했다. 과연 세계 최대 규모의 GPU를 투입한 모델이 어떤 성능을 보일지 관심이 집중된다.

머스크 CEO는 6일(현지시간) X(트위터)를 통해 “곧 그록 3를 출시할 것”이라고 밝혔다. 구체적인 시점은 언급하지 않았다.

이는 지난주 테크크런치가 “머스크는 그록 3를 2024년 내 출시한다고 말했으나, 결국 연기됐다”라고 보도한 것에 응답으로 보인다.

그는 지난해 7월 X를 통해 “xAI의 멤피스 데이터센터에서 10만개의 'H100' GPU를 사용해 훈련한 그록 3를 연말 출시할 예정이며, 이는 매우 특별한 모델이 될 것”이라고 발표했다. 하지만 8월 인터뷰에서는 출시 시점을 “운이 좋다면 2024년”으로 수정하며 불확실성을 드러냈다.

https://twitter.com/elonmusk/status/1875357350393246114

이번 발표에서 머스크 CEO는 “그록 3는 그록 2보다 10배 더 많은 컴퓨팅 파워로 사전 학습됐다”라고 강조했다. 알려진 대로 그록 3는 테네시주 멤피스에 건설된 xAI의 슈퍼컴퓨터 ‘콜로서스’를 통해 학습했다.

이제까지 생성 AI 모델의 성능은 컴퓨팅 파워와 학습 데이터의 양에 크게 의존했다. 하지만 최근 스케일링 법칙이 벽에 막히며 사전 훈련으로는 기대한 만큼의 성능 향상을 얻기 어렵다는 분석이 나왔다.

이 때문에 그록 3도 성능 부족으로 출시 기한을 넘겼을 것이라는 추측이 나왔다. 그러나 머스크 CEO는 이에 대한 언급은 피하고 단지 투입한 GPU의 숫자를 강조한 것이다.

물론 10만장의 GPU는 현재 세계 최고 수준으로, 이 정도의 컴퓨팅 인프라에서도 스케일링 법칙의 한계가 드러났는지는 미지수다.

따라서 새로 등장할 그록 3가 오픈AI 모델이나 최근 등장한 구글의 '제미나이 2.0' 이상의 성능을 보인다면, 양상은 또 달라질 수 있다. 이런 점에서 그록 3의 출시는 상당한 주목을 받고 있다.

또 머스크 CEO는 그록 3 출시 이후 차세대 모델 훈련을 위해 콜로서스 GPU를 현재 10배인 100만개 이상 확장할 계획으로 알려져 있다.

---

## [10] o1 같은 추론 모델 구축용 '강화 학습 프레임워크' 잇달아 등장

URL: https://www.aitimes.com/news/articleView.html?idxno=166843

![이미지](https://cdn.aitimes.com/news/photo/202501/166843_181470_90.png)

*푸단대의 로드맵 프레임워크 설명도 (사진=arXiv)*

오픈AI의 'o1'을 따라잡으려는 중국의 시도가 이어지고 있다. 이번에는 일반적인 지식 증류(distillation) 방법 대신, 추론 모델 구축을 위한 강화 학습(RL) 프레임워크가 잇달아 공개됐다.

중국 푸단대학교와 상하이 인공지능(AI) 연구실은 최근 강화 학습의 관점에서 o1을 재생산하기 위한 로드맵을 개발했다고 발표했다. 논문의 제목은 '검색 및 학습의 확장: 강화 학습 관점에서 o1을 재생산하기 위한 로드맵'이다.

말 그대로 o1 모델을 구축하기 위해 검색과 강화 학습을 확장한다는 내용이다. 실제로 이 프레임워크는 ▲정책 초기화 ▲보상 설계 ▲검색 ▲학습 등 네가지 핵심 구성 요소에 초점을 맞췄다.

정책 초기화를 통해 모델은 인간과 유사한 추론 행동을 개발, 복잡한 문제에 대한 솔루션을 효과적으로 탐색할 수 있는 능력을 갖추게 된다.

보상 설계는 프로세스 보상과 같은 기술을 사용해 중간 단계를 검증하는 것으로, 오픈AI가 스스로 o1의 핵심이라고 밝힌 강화 학습(RL)과 검색을 강화하기 위한 것이다.

검색 전략으로는 '몬테카를로 트리 탐색(MCTS)' 및 '빔 탐색'으로 고품질 솔루션을 생성하는 데 초점을 맞췄다. 검색을 통해 생성한 데이터는 RL을 통해 모델 정책을 반복적으로 개선한다.

이런 요소를 통합, 결국 추론 기능을 향상하는 검색과 학습 간의 시너지를 보여준다는 내용이다. 또 이 방식은 수동으로 큐레이팅된 데이터에 대한 의존도를 줄여, 추론 기능을 향상하면서 리소스를 효율적으로 만든다고 설명했다.

연구진도 "오픈AI는 o1의 주요 기술이 RL이라고 주장했다"라며 "최근 많은 연구에서 지식 증류와 같은 대체 접근 방식을 사용해 o1의 추론 스타일을 모방하지만, 그 효과는 교사 모델의 역량 한계에 의해 제한된다"라고 지적했다. 이에 따라 RL의 관점에서 o1을 달성하기 위한 로드맵을 분석했다고 설명했다.

이 프레임워크를 적용한 결과, 추론 정확도와 일반화에서 큰 개선이 이뤄졌다고 밝혔다.

예를 들어, 프로세스 보상은 까다로운 추론 벤치마크에서 작업 성공률을 20% 이상 증가했다. MCTS와 같은 검색 전략은 고품질 솔루션을 생성하는 데 효과적이며, 구조화된 탐색을 통해 추론을 개선했다. 또 검색으로 생성된 데이터를 사용한 반복 학습을 통해 모델은 기존 방법보다 적은 매개변수로 고급 추론 기능을 달성할 수 있었다는 설명이다.

연구진은 "이런 결과는 o1과 같은 모델의 성능을 복제하기 위해서는 RL이 핵심이며, 일반적으로 이 프레임워크가 유효하다는 것을 보여준다"라고 전했다.

칭화대학교 등으로 구성된 연구진도 RL을 통한 추론 모델 개발법을 오픈 소스로 공개했다. '프라임(Process Reinforcement through IMplicit Rewards)'이라는 접근 방식 역시 RL 프로세스 보상을 통해 모델 추론을 강화하는 것이다.

이를 통해 연구진은 '큐원2.5-매스-7B(Qwen2.5-Math-7B)'를 베이스로 강력한 추론 모델인 '유러스-2-7B-프라임(Eurus-2-7B-PRIME)'을 개발할 수 있었다고 전했다. 이 모델은 AIME(수학경시대회) 벤치마크에서 26.7%를 기록, 'GPT-4o'와 소스 모델인 큐원2.5-매스를 능가했다고 밝혔다.

이처럼 o1 출시 후 최근 중국에서는 잇달아 o1을 따라잡기 위한 연구 결과가 쏟아지고 있다.

지난주에는 텐센트와 상하이 자오퉁대학교 연구진이 추론 전문 모델의 과잉 사고 문제를 해결하기 위한 학습 방법을 공개했으며, 난징대학교 등도 추론 중 토큰 사용량을 최소화하면서 정확도를 유지할 수 있는 추론 프레임워크 논문을 공개했다.

또 딥시크와 알리바바, 문샷 등 주요 AI 기업도 추론 전문 모델을 공개해 눈길을 끌었다. 그중 딥시크의 'V3'는 오픈 소스 역대 최고 규모와 성능으로 주목받았다.

---


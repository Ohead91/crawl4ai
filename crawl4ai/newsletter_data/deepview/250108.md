
---
title: ⚙️ Progress & Predictions 2025: Robots, AVs and technical advancements
url: https://www.thedeepview.co/p/progress-predictions-2025-robots-avs-and-technical-advancements
date: 250108
source: deepview
crawled_at: 2025-01-10T15:38:35.033771
---

[![The Deep View logo](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/d2f6c485-9e1f-4efe-8972-c44827c29219/thumb_Untitled_design__1_.png)The Deep View](https://www.thedeepview.co/p/</>)
Login[Join Free](https://www.thedeepview.co/p/</subscribe>)
0
  * [The Deep View](https://www.thedeepview.co/p/<../>)
  * Posts
  * ⚙️ Progress & Predictions 2025: Robots, AVs and technical advancements


# ⚙️ Progress & Predictions 2025: Robots, AVs and technical advancements
![Author](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/user/profile_picture/35102f9d-8eba-4d2a-bcad-ae594f61c0d8/thumb_805a1f35-336c-4e7f-9092-112537dcf9a1.jpg)
[Ian Krietzberg](https://www.thedeepview.co/p/<https:/www.twitter.com/IKrietzberg?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) January 08, 2025 
[](https://www.thedeepview.co/p/<https:/www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.thedeepview.co%2Fp%2Fprogress-predictions-2025-robots-avs-and-technical-advancements>)[](https://www.thedeepview.co/p/<https:/twitter.com/intent/tweet?url=https%3A%2F%2Fwww.thedeepview.co%2Fp%2Fprogress-predictions-2025-robots-avs-and-technical-advancements&via=IKrietzberg>)[](https://www.thedeepview.co/p/<https:/www.threads.net/intent/post?text=https%3A%2F%2Fwww.thedeepview.co%2Fp%2Fprogress-predictions-2025-robots-avs-and-technical-advancements>)[](https://www.thedeepview.co/p/<https:/www.linkedin.com/sharing/share-offsite?url=https%3A%2F%2Fwww.thedeepview.co%2Fp%2Fprogress-predictions-2025-robots-avs-and-technical-advancements>)
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/870aa7c5-bf23-4507-91f3-9e4cf029279c/Banner_Image-3.jpg?t=1735051945)
**Good morning, and welcome to part 3 of our special edition series.**
In 2023, Waymo [delivered](https://www.thedeepview.co/p/<https:/waymo.com/blog/2023/12/dear-waymo-community-reflections-from-this-year-together?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) more than 700,000 trips. 
In 2024, the self-driving firm delivered more than four million rides. 
The past 12 months saw a lot of improvements in autonomous vehicles, robotics and the AI models that make them possible. But it also affirmed certain limitations, which grounds the technology in a rather interesting way. 
Let’s get into it. 
— Ian Krietzberg, Editor-in-Chief, The Deep View 
# **The rise of Waymo**
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/6d8b2e49-53c7-4340-b06d-418f69a7b6dc/2.jpg?t=1734452638)
Source: Waymo
By the end of 2024, [Waymo had expanded its operations](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/the-public-health-crisis-of-ai>) to serve a total of 500 square kilometers across its three major hubs: San Francisco, Los Angeles and Phoenix. And with those arenas on lock, the self-driving firm spent the year laying the foundation for an expansion into Austin, Atlanta, Miami and Tokyo set to take place in 2025. 
If you measure the rate of self-driving progress by Waymo alone, the company, at this point, has arguably established an operable robotaxi business. 
But scale, even for Waymo, remains a challenge at two different levels.
The first, of course, is cost. Waymo’s financials aren’t clear — parent company Google records Waymo’s numbers beneath its “Other Bets” umbrella, which disguises the full scope of Waymo’s revenue and losses. But, going just by the unit itself, the [loss seems to be narrowing](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/the-public-health-crisis-of-ai>); in the third quarter of 2024, the “Other Bets” unit reported $388 million in revenue and a $1.12 billion loss, a far smaller loss than the $1.98 billion reported the year before. 
The cost of each vehicle, meanwhile, is rumored to be somewhere in the region of $200,000.
All in, you certainly have a loss-making business. But the days of robotaxi revenues being perpetually on the horizon are over, at least for Waymo; the firm might well continue losing money for years, but it is finally generating a return. And investors seem sold on its potential; Waymo in 2024 closed a $5.6 billion funding round, led, unsurprisingly, by Google. 
You might call 2024 the year that Waymo pulled ahead. 
The other challenge of scale relates to safety. Thus far, Waymo has avoided any severe accidents and lawsuits, though it _is_[being investigated](https://www.thedeepview.co/p/<https:/www.cnn.com/2024/05/14/business/self-driving-cars-waymo-zoox-regulators-investigating/index.html?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) by the National Highway Traffic Safety Administration (NHTSA). It remains unclear how many miles are clocked by each vehicle in its fleet, the average distance of each ride and the [role and scope of its remote operators](https://www.thedeepview.co/p/<https:/arstechnica.com/cars/2024/05/on-self-driving-waymo-is-playing-chess-while-tesla-plays-checkers/?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>); the [numbers have yet to scale to a point](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/the-complicated-statistics-behind-safe-self-driving-cars>) where they can support sweeping safety comparison against humans, and self-driving researchers remain skeptical that Waymo’s safety record will scale in kind with its expansion. 
# Sinking the competition
Now, those two issues of cost and safety have proved too much to overcome for other players in the robotaxi race. Cruise — until the end of 2023, Waymo’s most prominent competitor — was forced in 2023 to shutter its operations after a Cruise robotaxi struck and dragged a pedestrian. The self-driving unit spent 2024 very slowly, very cautiously, making its way back to the road, [only to be dissolved by parent General Motors](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/google-s-gemini-2-0-era-is-here>) due to issues of cost and too much competition. 
GM will be merging Cruise with its internal teams next year to pursue advanced driver-assist tech, rather than straight-up robotaxis. 
Amazon-owned Zoox, meanwhile — the only one making a robotaxi without traditional car controls — is [rolling slowly and](https://www.thedeepview.co/p/<https:/zoox.com/journal/zoox-robotaxi-in-san-francisco?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) steadily. In 2024, the firm began testing its services in San Francisco and expanded its Las Vegas test, with plans to begin welcoming public riders sometime in 2025. 
And that brings us to Tesla, the Elon Musk-owned company that has been steadily rolling out software updates to its misnamed Full-Self Driving software for months. See, Tesla is a bit of a weird case here, especially against a backdrop of robotaxi firms. The company, for years, now, has offered two different driver-assist softwares: Autopilot and Full-Self Driving (FSD). Despite the implications of their names, neither of these offers legitimate self-driving; both require the hands-on, eyes-on attention of the driver. 
This conflict between Tesla’s marketing and the realities of its software is at the root of more than a dozen lawsuits — not counting the multiple federal and state investigations — [that are ongoing against Tesla and Musk](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/character-ai-sued-for-mental-health-decline-in-teenage-users-allegedly-encouraged-user-to-murder-his>). 
NHTSA [warned Tesla](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/character-ai-sued-for-mental-health-decline-in-teenage-users-allegedly-encouraged-user-to-murder-his>) in November to ease up on the self-driving hype in its marketing materials online. 
Musk in [October unveiled Tesla’s own stab at a robotaxi](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/tesla-stock-plummets-following-robotaxi-unveil>), the flashy, two-seater Cybercab, which he said will enter production in 2026. In the meantime, he said that the Model 3 and Y will go fully autonomous in Texas and California next year, though how he’ll conquer the regulatory (and technical) burdens there remains unclear. 
## **Robocars and robo-workers**
But Musk, to his credit, has more than one autonomous basket to draw from. I’m talking about Optimus, his humanoid robot. 
While progress is hard to measure, externally — all we’ve really seen this year is a few videos of the robot walking around — those videos show a big leap in progress compared to 2021, when Musk revealed the Optimus robot, which, at the time, was a man in a suit … 
As of October, the [robot was capable](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/competition-and-power-consolidation-in-ai>) of exploring unseen spaces autonomously, climbing stairs and carrying heavy objects without overheating, according to Tesla. 
But Figure, Tesla’s big humanoid robotics competition, [spent the year demonstrating](https://www.thedeepview.co/p/<https:/x.com/figure_robot?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) its humanoid robots in use at a BMW factory, a step Optimus has yet to take. Earlier in the year, Figure partnered with OpenAI to bring ChatGPT to its robotic interfaces, and in February, closed a $675 million funding round at a $2.6 billion valuation. 
Figure plans to develop and deploy [billions of humanoid](https://www.thedeepview.co/p/<https:/www.figure.ai/master-plan?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) robots to take care of the jobs humans don’t like doing, and, eventually, to explore space. 
Researchers, meanwhile, spent the year working to combine advancements in generative AI and 3D mapping with advanced robotics; the early results, [according to a Johns Hopkins team](https://www.thedeepview.co/p/<https:/www.jhuapl.edu/news/news-releases/240822-human-robot-teaming?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>), are promising. The development of a generative AI “agent” in August enabled a dog-shaped robot to respond to natural language commands and navigate its surroundings. A possible result is a robotic assistant for battlefield medics.
And that brings us right around to the underlying software, the Large Language Models (LLMs) that are making all this possible. 
## **The wall**
We talked earlier this week about agents, which essentially marks a push away from single models and toward model systems. This has been the industry’s attempt to overcome the fundamental limitations of language models, because, for all the releases this year — OpenAI o1/o3, Google’s Gemini 2.0, Llama 3.3, etc. — hallucination (or confabulation) and bias remains a part of the architecture. 
Attempts to achieve reliability in 2024 got more creative. IBM, for instance, is [pushing advances in small language models](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/ibm-and-the-road-to-achieving-ai-efficiency>) intended to be combined with enterprise data, alongside literal “guardian” models to ensure safety and reliability. Contextual AI, which has always been focused on systems, [developed a new observability model.](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/interview-a-new-approach-to-ai-evaluation>) Enterprise startup Writer is [working on two paradigms](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/smaller-but-deeper-writers-secret-weapon-to-more-powerful-ai>); wider, more general-purpose models, and smaller (but deeper) “reasoning” models. 
And indeed, “reasoning” models have become all the buzz. 
OpenAI’s got o1 (and now, o3, but it’s not clear when it’ll actually be open for use) and Google’s got Gemini 2.0 Flash Thinking, both of which take advantage of chain-of-thought ‘reasoning’ to better answer queries. 
Still, the end of 2024 marked a point where the industry seemed to accept that scale — i.e., increasing the quantity of compute and training data — [is not enough to somehow morph a language model into a more powerful,](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/openai-others-dealing-with-diminishing-returns-in-current-architecture>) less-flawed system. Companies increasingly turned to combinations of synthetic and organic data, focusing on data that is better organized and of higher quality, while others — specifically in the enterprise — went all-in on hyper-specific models, trained on company data and designed to accomplish specific tasks, something that reduces the risk of hallucination. 
[Even the unveiling of OpenAI’s o3 model](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/openai-unveils-powerful-new-model>) acknowledges the fact that scale alone is not enough; though we know little about the system, we do know that the real boost in performance has to do more with its internal Chains of Thought rather than pure scale. 
And even with o3, GPT-5 isn’t here, yet, and doesn’t seem to be on its way. 
A new paradigm remains out of reach; the work, instead, has turned into optimizing the current paradigm. 
And there is likely a long way they can take it. 
But, looking at the benchmarks, the rate of progress has slowed down. The kind of exponential growth notched in early 2023 didn’t really appear in 2024 (o3 might mark a difference here, but nothing about the model has been independently verified by external researchers). Growth has become incremental, but that doesn’t mean growth hasn’t occurred. 
**Dr. John Licato, an associate professor of computer science** at the University of South Florida, told me that “the danger of hallucination is still there. I don’t really feel it’s gotten much better in the past year.”
  * “The fundamental arguments for why hallucination is inevitable are still there … I’m on the side where I’m less and less interested even in the thought of AGI (artificial general intelligence). The definitions are never consistent, even within the people who talk about it. It’s just too inconsistent, not rigorous.”
  * “The way that I’ve seen progress in this space has kind of been … you know when you’re in high school your sports ability doubles, and then you get to Olympic level stuff and their differences are measured in a quarter of a second. It seems that that’s what’s happening. In terms of absolute numbers, the improvements of the past 6 months are lower than the 6 months preceding it, because**that gets it close to the theoretical maximum** … I still see that as a significant increase.”


“I think we also want to be careful to disparage incremental improvements. That’s how science works. Incremental improvements. If you keep trying to pursue the kind of progress where you reinvent the wheel every time, you’re not going to make any progress. When you actually pursue incremental improvements, it’ll look like something dramatic when you reach a checkpoint. Transformers, the rest of the world saw that as a huge leap forward, but it really was an incremental improvement.”
John Licato 
**A look back at related stories we’ve done in the past year:**
  * [Gary Marcus on fighting for a positive AI future](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/gary-marcus-on-fighting-for-a-positive-ai-future>)
  * [Eric Xing and the age of AI empowerment](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/eric-xing-and-the-age-of-ai-empowerment>)
  * [MIT professor on achieving fair AI ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/mit-professor-on-achieving-fair-ai>)


**Johns Hopkins’ Dr. Bart Paulhamus** said in a recent media briefing that human-robot teaming will have a significant “impact” in 2025, due to the convergence of two technologies: generative AI and robotics. 
  * “The next wave is AI agents, powered by generative AI. And those … will be able to make decisions and take action, and that’s exactly what we want robots to do,” Paulhamus said. 
  * “This agentic AI, and the ability to tap into perception tools and movement tools and grasping tools, but also its ability to understand its environment and take action with that. So I think this is really exciting.” The Johns Hopkins team, plus a number of other research institutions, is focused on embodied AI, bringing the best advancements in generative tech to the physical world. 


**Dr. John Bates, the CEO of SER,** expects new developments — architectures beyond deep learning — to appear in 2025. 
  * "We've been pursuing AI or the concept of it since the 1950s, but what we truly mean by the term today refers to intelligent programs that can reason. We’re not there yet, and philosophers and neuroscientists might argue that we never will be. Currently, everything we're doing revolves around Deep Learning. This involves using artificial neural networks to identify patterns and apply learned rules to navigate specific domains.”
  * “We will almost certainly see new developments emerging — perhaps genetic models, for instance, which operate differently from neural networks or Deep Learning approaches. The key point is that what we currently have is not truly reasoning in any sense; it’s AI trained on data. We’ve defined what good reasoning looks like, and as a result, the AI replicates that behavior.”


“In a sense, none of this is the main concern — what truly matters is whether these technologies help us and prove to be useful. **I believe 2025 will be a pivotal year for a quantum leap in AI, potentially offering humanity not just one or two useful advancements, but a genuine game-changer — perhaps in** fields like medicine or transportation, though we can't say for certain yet,” he said. 
“For me — and perhaps for many others — it would be genuine home robots that can handle tasks like ironing and cleaning the apartment that would be genuinely beneficial,” Bates added. “Achieving this is quite challenging, but perhaps 2025 will be the year we finally see it come to fruition.”
**Dr. Gary Marcus**[wrote](https://www.thedeepview.co/p/<https:/garymarcus.substack.com/p/o3-agi-the-art-of-the-demo-and-what?utm_source=post-email-title&publication_id=888615&post_id=153450669&utm_campaign=email-post-title&isFreemail=true&r=3raqcm&triedRedirect=true&utm_medium=email>)**that, in 2025** , “a lot of AI influencers and maybe some companies are going to claim we have reached AGI. Almost nobody will give you their definition.”
“I am not saying we will never get AGI,” he added. “I am saying that many basic problems haven’t been solved. And I am saying, as every roboticist on the planet knows, that we shouldn’t take demos seriously, until purported products are actually released and subjected to outside scrutiny.”
In April, he wrote a list of things that ordinary people do that he doubts AI will be able to do by the end of 2025. Here are a few of them:
  * Watch a previously unseen mainstream movie and be able to follow plot twists, know when to laugh and be able to summarize it without giving away any spoilers or making up anything that didn’t actually happen.
  * Drive off-road vehicles, without maps, across streams, around obstacles such as fallen trees, and so on.
  * Write engaging biographies and obituaries without obvious hallucinations that aren’t grounded in reliable sources.


**A look back at related stories we’ve done in the past year:**
  * [Language and thought are not the same thing](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/language-and-thought-are-not-the-same-thing>)
  * [The dark side of scaling generative AI scaling datasets](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/the-dark-side-of-scaling-generative-ai-datasets>)
  * [The complicated statistics behind self-driving cars](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/the-complicated-statistics-behind-safe-self-driving-cars>)


![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/37cba1f9-38e5-44a5-8eb3-ea3643ea3893/image.png?t=1734644403)
Here’s the thing about AGI. I’m coming around to the idea most recently espoused by Sam Altman that we’ll achieve AGI sooner than we expect, and it’ll mean much less than anyone thinks. 
I think this will happen, not because we will crack a hitherto unachievable breakthrough, but because the definition of AGI will become steadily cheapened until it becomes in-reach. 
I take AGI to mean a general intelligence, a system capable of dynamic, flexible reasoning. It doesn’t have to be human-like at all, but it needs to possess a few characteristics that humans possess: flexibility, adaptability and generalization. 
Essentially, my big AGI marker is simple; if someone can develop a — transparent — system, with _very little training data_ , that is able to apply its “knowledge” reliably across novel situations, then I would say we might have a general intelligence on our hands. Because machine learning is machine learning, I do not expect this to happen. In fact, I would be shocked if we see something like this at any point soon; the latest innovations, o3 included, are all focused on leveraging massive quantities of training data. Smaller models tend to perform significantly worse across the so-called reasoning benchmarks that have become so important to the developers, which is an unsurprising result of the probabilistic architecture of machine learning algorithms (more data equals better probabilities). 
Something like this would seem to require models to actually understand their output, which would raise a lot of questions and today seems massively inaccessible. 
So that won’t happen in 2025. 
But through more training data, more compute and Chain of Thought explorations, large-scale models, albeit inefficiently and without consistent reliability, will become increasingly capable of mimicking that understanding, meaning their corpus of training data that enables it will become less and less important to some of the people using these products. 
Brute force is good enough for a lot of folks, and we’ll see a refining of brute force intelligence mimicry next year. Because of that, we won’t be anywhere close to an actual AGI, but if companies begin to measure “AGI” by a definition that looks only at what it is sometimes able to do and ignores _how_ it is able to do so, I wouldn’t be surprised if some actors declare that AGI has been achieved next year. 
This will create an interesting dynamic because research into the tech won’t stop; this is, after all, what the entire field has been pushing toward for decades. 
I do expect to see more grounded advancements in the models that make robotics possible, and I do think that different iterations of AI-enabled robots will become quite popular next year. They will be grounded by frustrating limitations — high cost, high energy use, short battery life, slow motion — but we’ll start to see them deployed in more arenas that remain high-risk or otherwise undesirable to humans. 
And on the self-driving front, Tesla will do what it has always done; sell hype, and fail to deliver. Waymo, meanwhile, will continue its expansion, although I expect that this expansion will falter sometime next year due to an incident (hopefully, a minor one) that seems unavoidable given the scope and speed of the company’s growth, crossed with the ongoing limitations of the architecture. 
I don’t think the hype will thin out next year. I think it will grow. But I expect the architectural limitations — bias, hallucination and compute cost/energy consumption — to persist, something that, in some sectors, might well blunt the excitement around adoption. 
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/6282f4a1-c194-46fc-8358-ec95f975c58a/10_AI_or_not.gif?t=1734644465)
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/1be584c2-6f15-4139-be8c-b9796a499692/Ski_REAL-min.jpg?t=1735052243)
### Which image is real?   
---  
  * [ ⬆️ Image 1 ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)
  * [ ⬇️ Image 2 ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)

  
[Login](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>) or [Subscribe](https://www.thedeepview.co/p/<https:/www.thedeepview.co/subscribe>) to participate in polls.   
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/a333696d-5637-4014-831b-76f53e13fc2e/Ski_FAKE-min.png?t=1735052249)
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/b446dc4e-a2fb-4bc3-8633-5e5e64d98426/Screenshot_2024-12-19_at_4.43.47_PM.png?t=1734644642)
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/f0e50f88-8a87-47cb-bfbc-e6c32cffe818/Real_or_Not_Template-4.jpg?t=1736303203)
## 🤔Your thought process: 
#### Selected Image 1 (Left): 
  * “The snow on the other one was smooth, even though people were walking on it.”


#### Selected Image 2 (Right): 
  * “Amount of snow on mountains doesn't match amount of snow in the foreground.”


![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/d585991c-d895-47a2-a116-917a79da340c/_FROMOURPARTNERS__5_.png?t=1719178830)
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/304c3211-9b4e-4f1b-b635-cc7a3c2f398e/Shrt_Stories_Blue_.png?t=1735051104)
  * **We’re well past the Turing Test** ; we need a Weizenbaum test for AI, Jack Stilgoe, a researcher at the University College London, [argues](https://www.thedeepview.co/p/<https:/www.science.org/doi/10.1126/science.adk0176?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>); we should be more concerned if something is needed, and if it is useful, than if it is a true artificial intelligence. 
  * **Ethicists at the University of Cambridge**[warn](https://www.thedeepview.co/p/<https:/www.cam.ac.uk/research/news/coming-ai-driven-economy-will-sell-your-decisions-before-you-take-them-researchers-warn?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) that, in the coming AI economy, AI agents will “sell your decisions before you take them,” through a thorough environment of influence and forecasting. 


![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/b2c61c06-a065-490b-b8f6-d18e0d159b7b/Broad_View.png?t=1719262668)
  * CES 2025: All the news, gadgets and surprises ([The Verge](https://www.thedeepview.co/p/<https:/www.theverge.com/2025/1/4/24307731/ces-2025-tvs-gaming-smart-home-wearables-news?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>)). 
  * Artificial intelligence advancing at ‘incredible pace,’ says Nvidia CEO ([Semafor](https://www.thedeepview.co/p/<https:/www.semafor.com/article/01/07/2025/artificial-intelligence-advancing-at-incredible-pace?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>)). 
  * Dow falls more than 170 points, Nasdaq loses nearly 2% as Nvidia leads tech sell-off ([CNBC](https://www.thedeepview.co/p/<https:/www.cnbc.com/2025/01/06/stock-market-today-live-updates.html?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>)). 
  * Research turns insecure license plate cameras into open source surveillance tool ([404 Media](https://www.thedeepview.co/p/<https:/www.404media.co/researcher-turns-insecure-license-plate-cameras-into-open-source-surveillance-tool/?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>)). 
  * Meta’s seismic shift ([The Information](https://www.thedeepview.co/p/<https:/www.theinformation.com/articles/metas-seismic-shift?rc=sbfmgm&utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>)). 


_If you want to get in front of an audience of 200,000+ developers, business leaders and tech enthusiasts,__[get in touch with us here](https://www.thedeepview.co/p/<https:/nnwdryn4me2.typeform.com/to/vzxAdnuI?utm_source=www.thedeepview.co&utm_medium=newsletter&utm_campaign=u-s-hospital-teams-up-with-suki-for-an-ai-assistant&_bhlid=899a446fb8590c3f4dab42c864907d7822828cad>)_ _._
# 💭 A poll before you go
Thanks for reading today’s edition of The Deep View! 
We’ll see you in the next one. 
### Here’s your view on the ethics of AI: 
45% of you think the ethical issues related to AI will get worse next year; 21% think, as regulation comes into force, they might start to get better. The rest aren’t sure. 
**Gonna get worse:**
  * _“Even with one party holding the reins in all three branches of government, polarization will make mitigation all but improbable.”_


**Gonna get worse:**
  * _“Ethical companies will behave ethically... however, there are MANY, MANY entities that don't care about ethics and will generate a lot of chaos, so I believe things will get worse, much worse.”_


### What do you think? Will AI advance like crazy in 2025, or will it falter?   
---  
  * [ Somewhere in the middle ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)
  * [ Robotaxis will become normal ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)
  * [ OpenAI will have AGI, but it won't mean anything ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)
  * [ No idea ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)
  * [ Something else ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)

  
[Login](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>) or [Subscribe](https://www.thedeepview.co/p/<https:/www.thedeepview.co/subscribe>) to participate in polls.   
![Get our free, 5-minute daily newsletter that makes you smarter about AI. Read by 450,000+ from Google, Meta, Microsoft, a16z and more.](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/d2f6c485-9e1f-4efe-8972-c44827c29219/thumb_Untitled_design__1_.png)
The Deep View
Get our free, 5-minute daily newsletter that makes you smarter about AI. Read by 450,000+ from Google, Meta, Microsoft, a16z and more.
Home
[Posts](https://www.thedeepview.co/p/</>)
[](https://www.thedeepview.co/p/<https:/twitter.com/thedeepview>)[](https://www.thedeepview.co/p/<https:/www.tiktok.com/@thedeepviewai>)[](https://www.thedeepview.co/p/<https:/www.instagram.com/thedeepview.co/>)[](https://www.thedeepview.co/p/<https:/rss.beehiiv.com/feeds/nswNBn2yqy.xml>)
© 2025 The Deep View.
[Privacy Policy](https://www.thedeepview.co/p/<https:/beehiiv.com/privacy>)[Terms of Use](https://www.thedeepview.co/p/<https:/beehiiv.com/tou>)



---
title: ⚙️ Progress & Predictions 2025: Robots, AVs and technical advancements
url: https://www.thedeepview.co/p/progress-predictions-2025-robots-avs-and-technical-advancements
date: 250108
source: deepview
crawled_at: 2025-01-10T15:38:57.707276
---

[![The Deep View logo](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/d2f6c485-9e1f-4efe-8972-c44827c29219/thumb_Untitled_design__1_.png)The Deep View](https://www.thedeepview.co/p/</>)
Login[Join Free](https://www.thedeepview.co/p/</subscribe>)
0
  * [The Deep View](https://www.thedeepview.co/p/<../>)
  * Posts
  * ⚙️ Progress & Predictions 2025: Robots, AVs and technical advancements


# ⚙️ Progress & Predictions 2025: Robots, AVs and technical advancements
![Author](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/user/profile_picture/35102f9d-8eba-4d2a-bcad-ae594f61c0d8/thumb_805a1f35-336c-4e7f-9092-112537dcf9a1.jpg)
[Ian Krietzberg](https://www.thedeepview.co/p/<https:/www.twitter.com/IKrietzberg?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) January 08, 2025 
[](https://www.thedeepview.co/p/<https:/www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.thedeepview.co%2Fp%2Fprogress-predictions-2025-robots-avs-and-technical-advancements>)[](https://www.thedeepview.co/p/<https:/twitter.com/intent/tweet?url=https%3A%2F%2Fwww.thedeepview.co%2Fp%2Fprogress-predictions-2025-robots-avs-and-technical-advancements&via=IKrietzberg>)[](https://www.thedeepview.co/p/<https:/www.threads.net/intent/post?text=https%3A%2F%2Fwww.thedeepview.co%2Fp%2Fprogress-predictions-2025-robots-avs-and-technical-advancements>)[](https://www.thedeepview.co/p/<https:/www.linkedin.com/sharing/share-offsite?url=https%3A%2F%2Fwww.thedeepview.co%2Fp%2Fprogress-predictions-2025-robots-avs-and-technical-advancements>)
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/870aa7c5-bf23-4507-91f3-9e4cf029279c/Banner_Image-3.jpg?t=1735051945)
**Good morning, and welcome to part 3 of our special edition series.**
In 2023, Waymo [delivered](https://www.thedeepview.co/p/<https:/waymo.com/blog/2023/12/dear-waymo-community-reflections-from-this-year-together?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) more than 700,000 trips. 
In 2024, the self-driving firm delivered more than four million rides. 
The past 12 months saw a lot of improvements in autonomous vehicles, robotics and the AI models that make them possible. But it also affirmed certain limitations, which grounds the technology in a rather interesting way. 
Let’s get into it. 
— Ian Krietzberg, Editor-in-Chief, The Deep View 
# **The rise of Waymo**
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/6d8b2e49-53c7-4340-b06d-418f69a7b6dc/2.jpg?t=1734452638)
Source: Waymo
By the end of 2024, [Waymo had expanded its operations](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/the-public-health-crisis-of-ai>) to serve a total of 500 square kilometers across its three major hubs: San Francisco, Los Angeles and Phoenix. And with those arenas on lock, the self-driving firm spent the year laying the foundation for an expansion into Austin, Atlanta, Miami and Tokyo set to take place in 2025. 
If you measure the rate of self-driving progress by Waymo alone, the company, at this point, has arguably established an operable robotaxi business. 
But scale, even for Waymo, remains a challenge at two different levels.
The first, of course, is cost. Waymo’s financials aren’t clear — parent company Google records Waymo’s numbers beneath its “Other Bets” umbrella, which disguises the full scope of Waymo’s revenue and losses. But, going just by the unit itself, the [loss seems to be narrowing](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/the-public-health-crisis-of-ai>); in the third quarter of 2024, the “Other Bets” unit reported $388 million in revenue and a $1.12 billion loss, a far smaller loss than the $1.98 billion reported the year before. 
The cost of each vehicle, meanwhile, is rumored to be somewhere in the region of $200,000.
All in, you certainly have a loss-making business. But the days of robotaxi revenues being perpetually on the horizon are over, at least for Waymo; the firm might well continue losing money for years, but it is finally generating a return. And investors seem sold on its potential; Waymo in 2024 closed a $5.6 billion funding round, led, unsurprisingly, by Google. 
You might call 2024 the year that Waymo pulled ahead. 
The other challenge of scale relates to safety. Thus far, Waymo has avoided any severe accidents and lawsuits, though it _is_[being investigated](https://www.thedeepview.co/p/<https:/www.cnn.com/2024/05/14/business/self-driving-cars-waymo-zoox-regulators-investigating/index.html?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) by the National Highway Traffic Safety Administration (NHTSA). It remains unclear how many miles are clocked by each vehicle in its fleet, the average distance of each ride and the [role and scope of its remote operators](https://www.thedeepview.co/p/<https:/arstechnica.com/cars/2024/05/on-self-driving-waymo-is-playing-chess-while-tesla-plays-checkers/?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>); the [numbers have yet to scale to a point](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/the-complicated-statistics-behind-safe-self-driving-cars>) where they can support sweeping safety comparison against humans, and self-driving researchers remain skeptical that Waymo’s safety record will scale in kind with its expansion. 
# Sinking the competition
Now, those two issues of cost and safety have proved too much to overcome for other players in the robotaxi race. Cruise — until the end of 2023, Waymo’s most prominent competitor — was forced in 2023 to shutter its operations after a Cruise robotaxi struck and dragged a pedestrian. The self-driving unit spent 2024 very slowly, very cautiously, making its way back to the road, [only to be dissolved by parent General Motors](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/google-s-gemini-2-0-era-is-here>) due to issues of cost and too much competition. 
GM will be merging Cruise with its internal teams next year to pursue advanced driver-assist tech, rather than straight-up robotaxis. 
Amazon-owned Zoox, meanwhile — the only one making a robotaxi without traditional car controls — is [rolling slowly and](https://www.thedeepview.co/p/<https:/zoox.com/journal/zoox-robotaxi-in-san-francisco?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) steadily. In 2024, the firm began testing its services in San Francisco and expanded its Las Vegas test, with plans to begin welcoming public riders sometime in 2025. 
And that brings us to Tesla, the Elon Musk-owned company that has been steadily rolling out software updates to its misnamed Full-Self Driving software for months. See, Tesla is a bit of a weird case here, especially against a backdrop of robotaxi firms. The company, for years, now, has offered two different driver-assist softwares: Autopilot and Full-Self Driving (FSD). Despite the implications of their names, neither of these offers legitimate self-driving; both require the hands-on, eyes-on attention of the driver. 
This conflict between Tesla’s marketing and the realities of its software is at the root of more than a dozen lawsuits — not counting the multiple federal and state investigations — [that are ongoing against Tesla and Musk](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/character-ai-sued-for-mental-health-decline-in-teenage-users-allegedly-encouraged-user-to-murder-his>). 
NHTSA [warned Tesla](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/character-ai-sued-for-mental-health-decline-in-teenage-users-allegedly-encouraged-user-to-murder-his>) in November to ease up on the self-driving hype in its marketing materials online. 
Musk in [October unveiled Tesla’s own stab at a robotaxi](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/tesla-stock-plummets-following-robotaxi-unveil>), the flashy, two-seater Cybercab, which he said will enter production in 2026. In the meantime, he said that the Model 3 and Y will go fully autonomous in Texas and California next year, though how he’ll conquer the regulatory (and technical) burdens there remains unclear. 
## **Robocars and robo-workers**
But Musk, to his credit, has more than one autonomous basket to draw from. I’m talking about Optimus, his humanoid robot. 
While progress is hard to measure, externally — all we’ve really seen this year is a few videos of the robot walking around — those videos show a big leap in progress compared to 2021, when Musk revealed the Optimus robot, which, at the time, was a man in a suit … 
As of October, the [robot was capable](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/competition-and-power-consolidation-in-ai>) of exploring unseen spaces autonomously, climbing stairs and carrying heavy objects without overheating, according to Tesla. 
But Figure, Tesla’s big humanoid robotics competition, [spent the year demonstrating](https://www.thedeepview.co/p/<https:/x.com/figure_robot?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) its humanoid robots in use at a BMW factory, a step Optimus has yet to take. Earlier in the year, Figure partnered with OpenAI to bring ChatGPT to its robotic interfaces, and in February, closed a $675 million funding round at a $2.6 billion valuation. 
Figure plans to develop and deploy [billions of humanoid](https://www.thedeepview.co/p/<https:/www.figure.ai/master-plan?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) robots to take care of the jobs humans don’t like doing, and, eventually, to explore space. 
Researchers, meanwhile, spent the year working to combine advancements in generative AI and 3D mapping with advanced robotics; the early results, [according to a Johns Hopkins team](https://www.thedeepview.co/p/<https:/www.jhuapl.edu/news/news-releases/240822-human-robot-teaming?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>), are promising. The development of a generative AI “agent” in August enabled a dog-shaped robot to respond to natural language commands and navigate its surroundings. A possible result is a robotic assistant for battlefield medics.
And that brings us right around to the underlying software, the Large Language Models (LLMs) that are making all this possible. 
## **The wall**
We talked earlier this week about agents, which essentially marks a push away from single models and toward model systems. This has been the industry’s attempt to overcome the fundamental limitations of language models, because, for all the releases this year — OpenAI o1/o3, Google’s Gemini 2.0, Llama 3.3, etc. — hallucination (or confabulation) and bias remains a part of the architecture. 
Attempts to achieve reliability in 2024 got more creative. IBM, for instance, is [pushing advances in small language models](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/ibm-and-the-road-to-achieving-ai-efficiency>) intended to be combined with enterprise data, alongside literal “guardian” models to ensure safety and reliability. Contextual AI, which has always been focused on systems, [developed a new observability model.](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/interview-a-new-approach-to-ai-evaluation>) Enterprise startup Writer is [working on two paradigms](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/smaller-but-deeper-writers-secret-weapon-to-more-powerful-ai>); wider, more general-purpose models, and smaller (but deeper) “reasoning” models. 
And indeed, “reasoning” models have become all the buzz. 
OpenAI’s got o1 (and now, o3, but it’s not clear when it’ll actually be open for use) and Google’s got Gemini 2.0 Flash Thinking, both of which take advantage of chain-of-thought ‘reasoning’ to better answer queries. 
Still, the end of 2024 marked a point where the industry seemed to accept that scale — i.e., increasing the quantity of compute and training data — [is not enough to somehow morph a language model into a more powerful,](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/openai-others-dealing-with-diminishing-returns-in-current-architecture>) less-flawed system. Companies increasingly turned to combinations of synthetic and organic data, focusing on data that is better organized and of higher quality, while others — specifically in the enterprise — went all-in on hyper-specific models, trained on company data and designed to accomplish specific tasks, something that reduces the risk of hallucination. 
[Even the unveiling of OpenAI’s o3 model](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/openai-unveils-powerful-new-model>) acknowledges the fact that scale alone is not enough; though we know little about the system, we do know that the real boost in performance has to do more with its internal Chains of Thought rather than pure scale. 
And even with o3, GPT-5 isn’t here, yet, and doesn’t seem to be on its way. 
A new paradigm remains out of reach; the work, instead, has turned into optimizing the current paradigm. 
And there is likely a long way they can take it. 
But, looking at the benchmarks, the rate of progress has slowed down. The kind of exponential growth notched in early 2023 didn’t really appear in 2024 (o3 might mark a difference here, but nothing about the model has been independently verified by external researchers). Growth has become incremental, but that doesn’t mean growth hasn’t occurred. 
**Dr. John Licato, an associate professor of computer science** at the University of South Florida, told me that “the danger of hallucination is still there. I don’t really feel it’s gotten much better in the past year.”
  * “The fundamental arguments for why hallucination is inevitable are still there … I’m on the side where I’m less and less interested even in the thought of AGI (artificial general intelligence). The definitions are never consistent, even within the people who talk about it. It’s just too inconsistent, not rigorous.”
  * “The way that I’ve seen progress in this space has kind of been … you know when you’re in high school your sports ability doubles, and then you get to Olympic level stuff and their differences are measured in a quarter of a second. It seems that that’s what’s happening. In terms of absolute numbers, the improvements of the past 6 months are lower than the 6 months preceding it, because**that gets it close to the theoretical maximum** … I still see that as a significant increase.”


“I think we also want to be careful to disparage incremental improvements. That’s how science works. Incremental improvements. If you keep trying to pursue the kind of progress where you reinvent the wheel every time, you’re not going to make any progress. When you actually pursue incremental improvements, it’ll look like something dramatic when you reach a checkpoint. Transformers, the rest of the world saw that as a huge leap forward, but it really was an incremental improvement.”
John Licato 
**A look back at related stories we’ve done in the past year:**
  * [Gary Marcus on fighting for a positive AI future](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/gary-marcus-on-fighting-for-a-positive-ai-future>)
  * [Eric Xing and the age of AI empowerment](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/eric-xing-and-the-age-of-ai-empowerment>)
  * [MIT professor on achieving fair AI ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/mit-professor-on-achieving-fair-ai>)


**Johns Hopkins’ Dr. Bart Paulhamus** said in a recent media briefing that human-robot teaming will have a significant “impact” in 2025, due to the convergence of two technologies: generative AI and robotics. 
  * “The next wave is AI agents, powered by generative AI. And those … will be able to make decisions and take action, and that’s exactly what we want robots to do,” Paulhamus said. 
  * “This agentic AI, and the ability to tap into perception tools and movement tools and grasping tools, but also its ability to understand its environment and take action with that. So I think this is really exciting.” The Johns Hopkins team, plus a number of other research institutions, is focused on embodied AI, bringing the best advancements in generative tech to the physical world. 


**Dr. John Bates, the CEO of SER,** expects new developments — architectures beyond deep learning — to appear in 2025. 
  * "We've been pursuing AI or the concept of it since the 1950s, but what we truly mean by the term today refers to intelligent programs that can reason. We’re not there yet, and philosophers and neuroscientists might argue that we never will be. Currently, everything we're doing revolves around Deep Learning. This involves using artificial neural networks to identify patterns and apply learned rules to navigate specific domains.”
  * “We will almost certainly see new developments emerging — perhaps genetic models, for instance, which operate differently from neural networks or Deep Learning approaches. The key point is that what we currently have is not truly reasoning in any sense; it’s AI trained on data. We’ve defined what good reasoning looks like, and as a result, the AI replicates that behavior.”


“In a sense, none of this is the main concern — what truly matters is whether these technologies help us and prove to be useful. **I believe 2025 will be a pivotal year for a quantum leap in AI, potentially offering humanity not just one or two useful advancements, but a genuine game-changer — perhaps in** fields like medicine or transportation, though we can't say for certain yet,” he said. 
“For me — and perhaps for many others — it would be genuine home robots that can handle tasks like ironing and cleaning the apartment that would be genuinely beneficial,” Bates added. “Achieving this is quite challenging, but perhaps 2025 will be the year we finally see it come to fruition.”
**Dr. Gary Marcus**[wrote](https://www.thedeepview.co/p/<https:/garymarcus.substack.com/p/o3-agi-the-art-of-the-demo-and-what?utm_source=post-email-title&publication_id=888615&post_id=153450669&utm_campaign=email-post-title&isFreemail=true&r=3raqcm&triedRedirect=true&utm_medium=email>)**that, in 2025** , “a lot of AI influencers and maybe some companies are going to claim we have reached AGI. Almost nobody will give you their definition.”
“I am not saying we will never get AGI,” he added. “I am saying that many basic problems haven’t been solved. And I am saying, as every roboticist on the planet knows, that we shouldn’t take demos seriously, until purported products are actually released and subjected to outside scrutiny.”
In April, he wrote a list of things that ordinary people do that he doubts AI will be able to do by the end of 2025. Here are a few of them:
  * Watch a previously unseen mainstream movie and be able to follow plot twists, know when to laugh and be able to summarize it without giving away any spoilers or making up anything that didn’t actually happen.
  * Drive off-road vehicles, without maps, across streams, around obstacles such as fallen trees, and so on.
  * Write engaging biographies and obituaries without obvious hallucinations that aren’t grounded in reliable sources.


**A look back at related stories we’ve done in the past year:**
  * [Language and thought are not the same thing](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/language-and-thought-are-not-the-same-thing>)
  * [The dark side of scaling generative AI scaling datasets](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/the-dark-side-of-scaling-generative-ai-datasets>)
  * [The complicated statistics behind self-driving cars](https://www.thedeepview.co/p/<https:/www.thedeepview.co/p/the-complicated-statistics-behind-safe-self-driving-cars>)


![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/37cba1f9-38e5-44a5-8eb3-ea3643ea3893/image.png?t=1734644403)
Here’s the thing about AGI. I’m coming around to the idea most recently espoused by Sam Altman that we’ll achieve AGI sooner than we expect, and it’ll mean much less than anyone thinks. 
I think this will happen, not because we will crack a hitherto unachievable breakthrough, but because the definition of AGI will become steadily cheapened until it becomes in-reach. 
I take AGI to mean a general intelligence, a system capable of dynamic, flexible reasoning. It doesn’t have to be human-like at all, but it needs to possess a few characteristics that humans possess: flexibility, adaptability and generalization. 
Essentially, my big AGI marker is simple; if someone can develop a — transparent — system, with _very little training data_ , that is able to apply its “knowledge” reliably across novel situations, then I would say we might have a general intelligence on our hands. Because machine learning is machine learning, I do not expect this to happen. In fact, I would be shocked if we see something like this at any point soon; the latest innovations, o3 included, are all focused on leveraging massive quantities of training data. Smaller models tend to perform significantly worse across the so-called reasoning benchmarks that have become so important to the developers, which is an unsurprising result of the probabilistic architecture of machine learning algorithms (more data equals better probabilities). 
Something like this would seem to require models to actually understand their output, which would raise a lot of questions and today seems massively inaccessible. 
So that won’t happen in 2025. 
But through more training data, more compute and Chain of Thought explorations, large-scale models, albeit inefficiently and without consistent reliability, will become increasingly capable of mimicking that understanding, meaning their corpus of training data that enables it will become less and less important to some of the people using these products. 
Brute force is good enough for a lot of folks, and we’ll see a refining of brute force intelligence mimicry next year. Because of that, we won’t be anywhere close to an actual AGI, but if companies begin to measure “AGI” by a definition that looks only at what it is sometimes able to do and ignores _how_ it is able to do so, I wouldn’t be surprised if some actors declare that AGI has been achieved next year. 
This will create an interesting dynamic because research into the tech won’t stop; this is, after all, what the entire field has been pushing toward for decades. 
I do expect to see more grounded advancements in the models that make robotics possible, and I do think that different iterations of AI-enabled robots will become quite popular next year. They will be grounded by frustrating limitations — high cost, high energy use, short battery life, slow motion — but we’ll start to see them deployed in more arenas that remain high-risk or otherwise undesirable to humans. 
And on the self-driving front, Tesla will do what it has always done; sell hype, and fail to deliver. Waymo, meanwhile, will continue its expansion, although I expect that this expansion will falter sometime next year due to an incident (hopefully, a minor one) that seems unavoidable given the scope and speed of the company’s growth, crossed with the ongoing limitations of the architecture. 
I don’t think the hype will thin out next year. I think it will grow. But I expect the architectural limitations — bias, hallucination and compute cost/energy consumption — to persist, something that, in some sectors, might well blunt the excitement around adoption. 
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/6282f4a1-c194-46fc-8358-ec95f975c58a/10_AI_or_not.gif?t=1734644465)
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/1be584c2-6f15-4139-be8c-b9796a499692/Ski_REAL-min.jpg?t=1735052243)
### Which image is real?   
---  
  * [ ⬆️ Image 1 ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)
  * [ ⬇️ Image 2 ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)

  
[Login](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>) or [Subscribe](https://www.thedeepview.co/p/<https:/www.thedeepview.co/subscribe>) to participate in polls.   
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/a333696d-5637-4014-831b-76f53e13fc2e/Ski_FAKE-min.png?t=1735052249)
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/b446dc4e-a2fb-4bc3-8633-5e5e64d98426/Screenshot_2024-12-19_at_4.43.47_PM.png?t=1734644642)
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/f0e50f88-8a87-47cb-bfbc-e6c32cffe818/Real_or_Not_Template-4.jpg?t=1736303203)
## 🤔Your thought process: 
#### Selected Image 1 (Left): 
  * “The snow on the other one was smooth, even though people were walking on it.”


#### Selected Image 2 (Right): 
  * “Amount of snow on mountains doesn't match amount of snow in the foreground.”


![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/d585991c-d895-47a2-a116-917a79da340c/_FROMOURPARTNERS__5_.png?t=1719178830)
![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/304c3211-9b4e-4f1b-b635-cc7a3c2f398e/Shrt_Stories_Blue_.png?t=1735051104)
  * **We’re well past the Turing Test** ; we need a Weizenbaum test for AI, Jack Stilgoe, a researcher at the University College London, [argues](https://www.thedeepview.co/p/<https:/www.science.org/doi/10.1126/science.adk0176?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>); we should be more concerned if something is needed, and if it is useful, than if it is a true artificial intelligence. 
  * **Ethicists at the University of Cambridge**[warn](https://www.thedeepview.co/p/<https:/www.cam.ac.uk/research/news/coming-ai-driven-economy-will-sell-your-decisions-before-you-take-them-researchers-warn?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>) that, in the coming AI economy, AI agents will “sell your decisions before you take them,” through a thorough environment of influence and forecasting. 


![](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/b2c61c06-a065-490b-b8f6-d18e0d159b7b/Broad_View.png?t=1719262668)
  * CES 2025: All the news, gadgets and surprises ([The Verge](https://www.thedeepview.co/p/<https:/www.theverge.com/2025/1/4/24307731/ces-2025-tvs-gaming-smart-home-wearables-news?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>)). 
  * Artificial intelligence advancing at ‘incredible pace,’ says Nvidia CEO ([Semafor](https://www.thedeepview.co/p/<https:/www.semafor.com/article/01/07/2025/artificial-intelligence-advancing-at-incredible-pace?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>)). 
  * Dow falls more than 170 points, Nasdaq loses nearly 2% as Nvidia leads tech sell-off ([CNBC](https://www.thedeepview.co/p/<https:/www.cnbc.com/2025/01/06/stock-market-today-live-updates.html?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>)). 
  * Research turns insecure license plate cameras into open source surveillance tool ([404 Media](https://www.thedeepview.co/p/<https:/www.404media.co/researcher-turns-insecure-license-plate-cameras-into-open-source-surveillance-tool/?utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>)). 
  * Meta’s seismic shift ([The Information](https://www.thedeepview.co/p/<https:/www.theinformation.com/articles/metas-seismic-shift?rc=sbfmgm&utm_source=thedeepview&utm_medium=newsletter&utm_campaign=progress-predictions-2025-robots-avs-and-technical-advancements>)). 


_If you want to get in front of an audience of 200,000+ developers, business leaders and tech enthusiasts,__[get in touch with us here](https://www.thedeepview.co/p/<https:/nnwdryn4me2.typeform.com/to/vzxAdnuI?utm_source=www.thedeepview.co&utm_medium=newsletter&utm_campaign=u-s-hospital-teams-up-with-suki-for-an-ai-assistant&_bhlid=899a446fb8590c3f4dab42c864907d7822828cad>)_ _._
# 💭 A poll before you go
Thanks for reading today’s edition of The Deep View! 
We’ll see you in the next one. 
### Here’s your view on the ethics of AI: 
45% of you think the ethical issues related to AI will get worse next year; 21% think, as regulation comes into force, they might start to get better. The rest aren’t sure. 
**Gonna get worse:**
  * _“Even with one party holding the reins in all three branches of government, polarization will make mitigation all but improbable.”_


**Gonna get worse:**
  * _“Ethical companies will behave ethically... however, there are MANY, MANY entities that don't care about ethics and will generate a lot of chaos, so I believe things will get worse, much worse.”_


### What do you think? Will AI advance like crazy in 2025, or will it falter?   
---  
  * [ Somewhere in the middle ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)
  * [ Robotaxis will become normal ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)
  * [ OpenAI will have AGI, but it won't mean anything ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)
  * [ No idea ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)
  * [ Something else ](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>)

  
[Login](https://www.thedeepview.co/p/<https:/www.thedeepview.co/login>) or [Subscribe](https://www.thedeepview.co/p/<https:/www.thedeepview.co/subscribe>) to participate in polls.   
![Get our free, 5-minute daily newsletter that makes you smarter about AI. Read by 450,000+ from Google, Meta, Microsoft, a16z and more.](https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/d2f6c485-9e1f-4efe-8972-c44827c29219/thumb_Untitled_design__1_.png)
The Deep View
Get our free, 5-minute daily newsletter that makes you smarter about AI. Read by 450,000+ from Google, Meta, Microsoft, a16z and more.
Home
[Posts](https://www.thedeepview.co/p/</>)
[](https://www.thedeepview.co/p/<https:/twitter.com/thedeepview>)[](https://www.thedeepview.co/p/<https:/www.tiktok.com/@thedeepviewai>)[](https://www.thedeepview.co/p/<https:/www.instagram.com/thedeepview.co/>)[](https://www.thedeepview.co/p/<https:/rss.beehiiv.com/feeds/nswNBn2yqy.xml>)
© 2025 The Deep View.
[Privacy Policy](https://www.thedeepview.co/p/<https:/beehiiv.com/privacy>)[Terms of Use](https://www.thedeepview.co/p/<https:/beehiiv.com/tou>)


